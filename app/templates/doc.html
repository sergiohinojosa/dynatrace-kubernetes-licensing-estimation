{% extends "base.html" %}
{% block title %}Documentation{% endblock %}
{% block body %}
<h3 style="text-align: center;">K8stimator - Documentation</h3>
<div class="jumbotron">
  <h4>How does the K8stimator work?</h4>
  The K8stimator only needs an API Token with access to "Read metrics (GET Requests to Metrics API v2)" and the tenant
  URL. The program will fetch all PGIs that lived in the specified time and ran in Kubernetes or OpenShift environments
  and mimic the DPS Algorithm stated in our documentation for <a target="_blank"
    href="https://docs.dynatrace.com/docs/shortlink/dps-containers#billing-granularity-for-pod-hour-consumption">Pod-hours</a>
  and <a target="_blank"
    href="https://docs.dynatrace.com/docs/manage/subscriptions-and-licensing/dynatrace-platform-subscription/host-monitoring#gib-hour-calculation-for-application-only-container-monitoring">Gib-hours</a>.
  It will do two queries per iteration, one for the Pod-hours another for the Gib-hours. The POD-hours are calculated
  with all pods that existed in the environment for the timeframe specified, the Gib-hours needs also the pods that
  existed in the enviroment but only those where an OneAgent technology existed, basically the PODs that were
  instrumented.
  <br><br>
  The resolution defines the datapoints or existance of a pod within the specified time, basically aswering the question
  how many times did PGI X lived in this timeframe with the specified resolution. For the Gib-hour query, the avg size
  of the memory is given and this will be rounded up to the next 0.25 Gib as stated in <a target="_blank"
    href="https://docs.dynatrace.com/docs/manage/subscriptions-and-licensing/dynatrace-platform-subscription/host-monitoring#gib-hour-calculation-for-application-only-container-monitoring">the
    algorithm used in our DPS documentation.</a>

  Each iteration calculates the sum of Pod-hours and Gib-hours for the timeframe. It also calculates the short-living
  pods for verifying the accuracy of the resolution since the recommended resolution is 1 hour and not 15 minutes (as
  stated in the documentation). This because of increasing performance (reducing the payload by 400%) and avoiding
  metrics limitations withouth impacting the accuracy of the estimation. Normally a resolution of 1h in big environments
  is not bigger that 1%. Meaning only 1% of the short-living pods existed one time (had one data point) during the specified resolution, with this approach is possible to increase the resolution knowing how big the impact is in the accuracy. However, is good to know that the impact is just a higher impact on the costs by calculating more data-points (Gib-hours) of the pod than they should which at the end will not result in unpredicted costs to the customer. Still the goal is to be as accurate as possible, but if there is an inaccuracy, this should be at least in higher costs and not the other way around.  
  <br><br>
  At the end of all the iterations, a sum will be done calculating the total ammount of Pod-hours and Gib-hours. Also a
  daily average will be calculated and a yearly estimation will be done. This assuming of course the dynamics of the
  enviroment stay linear through the whole year.
  <br><br>
  <h4>I saw a warning in the iterations! what do I do? ⚠️</h4>
  A warning means that the data is capped due the amount of data fetched. This will lead to unaccurate results. This can
  be due the lenght of the iteration and/or the resolution. Also the older the data is, higher the aggregation on the
  metrics are and the server has more trouble to fetch data with a higher resolution. <br>
  Try to adapt the "from timeframe" as closer to today. If we are in March, I'd recommend to use this date 2024-02-01
  and iterate 4 to 8 times for 7 days per iteration with a resolution of 1 hour. Meaning you'll fetch the data for one
  or two months.
  Be carefull that the "from_timeframe" + "the iterations" x "days per iteration" does not surpasses today.
  <br><br>
  <h4>Why do we need an Iterative approach?</h4>
  In order to be able to calculate and fetch data from big Kubernetes or OpenShift environments with tausends, hundreds
  of tausends or even millions of pods, we need to split the timeframe with an iterational approach. Specially because
  in order to be sure that the fetched PGIs were actually PODs where the OneAgent existed, we need to merge the PGIs to
  its Services which by nature is a one-to-many relationship. We do this with an API :parent operation. Doing this
  assertion, the queried entities will easily explode into the millions, hence an iteration approach that is
  customizable is the most appropiate that can be adapted to the size and dynamic of any environment.
  <br><br>
  <h4>Does it work with Managed Clusters?</h4>
  Yes, this program is running at the moment in a replica set in a GKE Cluster with no connection to our corporate VPN. Solution Enginners can run this programm as an interim solution in a docker container for calculating consumption for Managed Clusters within VPN. For guidance contact Sergio.
  <br><br>
  <h4>More information about Kubernetes Observability and Application Observability consumption</h4>
  <ul>
    <li><h5><a target="_blank"
      href="https://docs.dynatrace.com/docs/shortlink/dps-containers#billing-granularity-for-pod-hour-consumption">
      POD-hour calculation</a></h5></li>
      <li><h5><a target="_blank"
        href="https://docs.dynatrace.com/docs/manage/subscriptions-and-licensing/dynatrace-platform-subscription/host-monitoring#gib-hour-calculation-for-application-only-container-monitoring">
        GiB-hour calculation</h5></a></li>      
  </ul>
  <!--
  Explain Resolution and datapoints. 15 Min is the resolution used in the algorithm for the DPS calculation, but for optimization we use 1hour since is accurate enough for the dynamics of Kuernetes environments. Meaninng the short-living instances live normally over 1 hour. Accepted resolutions are 15m, 1h, 6h and 1d
-->
<br>
<h4>Whats the techstack of the K8stimator?</h4>
The K8stimator was born within a simple Python script that turned into a somewhat full fledged Python app including Flask Web technology and Caching for asynchronous management of data. If you want to contribute, that's great! any help is welcome.
Take a look at the source code, here is the github repository: <a target="_blank" href="https://github.com/sergiohinojosa/dynatrace-kubernetes-licensing-estimation">https://github.com/sergiohinojosa/dynatrace-kubernetes-licensing-estimation</a>

</div>
{% endblock %}